%!TEX encoding = UTF8
%!TEX root = 0-notes.tex

\chapter{Variables aléatoires}

Dans toute la suite, $\Omega$ est un univers fini, et $P$ est une loi de probabilité.

\section{Introduction}

\dfn{Variable aléatoire}{
	Considérons une fonction réelle $f$ sur $\Omega$.
	On appelle $f$ une \emph{variable aléatoire} et on pose
		\begin{align*}
			P\bigl( f = k \bigr) :=&~ P\bigl( \bigset{ \omega \in \Omega \tq f(\omega)=k}\bigr), \\
								=&~ \sum_{\omega \in \Omega \tq f(\omega)=k} P(\omega).
		\end{align*}
}{dfn:var-alea}

\nt{
	Une variable aléatoire n'est ni une variable, ni aléatoire.
}

\notations{
	On note habituellement une variable aléatoire par la lettre $X$, et on décrit la fonction par des mots.
}

\ex{}{
	On lance deux D6, dés à 6 faces, l'un après l'autre et on note les résultats obtenus dans l'ordre.
	L'univers des issues est $\Omega = \bigset{ (i, j) \tq 1 \leq i, j \leq 6}$. Par équiprobabilité, on a $P(\omega) = \dfrac1{36}$ pour chaque issue $\omega$ de l'uniers $\Omega$.
	
	Posons désormais les variables aléatoires $X$ : « la valeur du premier résultat », et $Y$ : « la valeur du deuxième résultat ».
	Ce sont bien des fonctions de $\Omega$ dans $\R$ (ou plus précisément dans $\{1 ; 2 ; 3 ; 4 ; 5 ; 6\}$).
	Ainsi $X\bigl((1,2)\bigr) = 1, Y\bigl((1,2)\bigr) = 2, X\bigl((6,5)\bigr) = 6,$ et $Y\bigl((6,5)\bigr) = 5$, par exemple.
	
	Par définition, on a 
		\begin{align*}
			P\bigl( X = 1 \big) &= P\bigl( (1,1) \bigr) + P\bigl( (1,2) \bigr) + \cdots + P\bigl( (1,6) \bigr) , \\
			&= \sum_{k=1}^{6} P\bigl( (1, k) \bigr), \\
			&= 6 \times \dfrac1{36} = \dfrac16.
		\end{align*}
	Par symétrie, on a idem pour $Y$ : $P(Y=1) = \dfrac16$.
}{ex:var-alea0}

\exe{}{
	Dans le contexte de l'exemple \ref{ex:var-alea0}, montrer que $P(X=k) = \dfrac16$ pour chaque $k=1, \dots, 6$.
}{exe:var-alea0}{
	Il y a exactement 6 issues vérifiant $X=k$ : ce sont $(k,1), (k,2), \dots,$ et $(k,6)$.
	Chacun a probabilité $\dfrac1{36}$, ce qui conclut.
}

\ex{}{
	On reprend l'expérience aléatoire de l'exemple \ref{ex:var-alea0}.
	Posons cette fois-ci la variable aléatoire $Z :$ « la somme des deux résultats ».
	C'est bien une fonction de $\Omega$ dans $\R$ (ou plus précisément dans $\{2 ; 3 ; \dots ; 12\}$).
	Par exemple, $Z\bigl((1,3)\bigr) = 4$ et $Z\bigl((6,2)\bigr) = 8$.
	
	Par définition, on a 
		\[ P\bigl( Z = 2 \big) = P\bigl( (1,1) \bigr) = \dfrac1{36}, \]
	car le couple $(1,1)$ est le seul dont la somme vaut $2$.
	Similairement,
		\begin{align*}
			P\bigl( Z = 3 \big) &= P\bigl( (1,2) \bigr) + P\bigl( (2,1) \bigr), \\
			&= \dfrac2{36} = \dfrac1{18}.
		\end{align*}
}{ex:var-alea}

\exe{}{
	Dans le contexte de l'exemple \ref{ex:var-alea}, donner $P(Z=4)$ et $P(Z=5)$.
}{exe:var-alea}{
	Les couples $(1,3), (2,2), (3,1)$ ont pour somme 4, et donc $P(Z=4) = \dfrac{3}{36}$.
	
	Les couples $(1,4), (2,3), (3,2), (4,1)$ ont pour somme 5, et donc $P(Z=5) = \dfrac{5}{36}$.
}

\exe{, difficulty=1}{
	Dans le contexte de l'exemple \ref{ex:var-alea}, montrer que $P(Z=k) = \dfrac{\min\bigset{k-1, 13-k}}{36}$ pour $k\in\{2 ; 3 ; \dots ; 12 \}$.
}{exe:var-alea2}{
	TODO
}

\thm{}{
	Soit $X$ une variable aléatoire et $x \neq y$ deux réels distincts.
	Alors 
		\[ P\bigl( X = x \cup X = y \bigr) = P\bigl( X = x) + P\bigl(X = y \bigr). \]
}{thm:disjoint-sum}

\exe{, difficulty=2}{
	Démontrer le théorème \ref{thm:disjoint-sum}.
}{exe:disjoint-sum}{
	L'ensemble des issues $\omega\in\Omega$ vérifiant $X(\omega) = x$ est nécessairement disjoint de l'ensemble des issues $\omega\in\Omega$ vérifiant $X(\omega)=y$ car $x$ et $y$ sont deux réels différents.
	Les événements $X=x$ et $X=y$ sont donc disjoints et la formule d'inclusion-exclusion conclut.
}


\dfn{Somme de variables aléatoires}{
	Soient $X, Y$ deux variables aléatoires, et $c\in\R$ un réel quelconque.
	On définit la somme $X+Y$ comme étant la fonction $(X+Y)(\omega) = X(\omega) + Y(\omega)$.
	On définit le produit $cX$ comme étant la fonction $(cX)(\omega) = c X(\omega)$.
}{dfn:X-ev}

\ex{}{
	En reprenant les exemples \ref{ex:var-alea0} et \ref{ex:var-alea}, on a en fait
		\[ Z = X+Y. \]
}{ex:somme-var-alea}

\nt{
	Les manipulations algébriques usuelles restents valides.
	Par exemple, $P(X = 1) \iff P(2X = 2) \iff P(2X + Y = 2 + Y)$.
}

\section{Espérance}

\dfn{Espérance}{
	Soit $X$ une variable aléatoire prenant ses valeurs dans un ensemble $S \subseteq\R$.
	Alors on pose
		\[ \E(X) := \sum_{x\in S} x P(X=x). \]
}{dfn:espérance}

\nt{
	On a également
		\[ \E(X) = \sum_{\omega\in\Omega} X(\omega) P(\omega). \]
}

\exe{,difficulty=2}{
	Démontrer la remarque ci-dessus.
}{exe:rem-alea}{
	TODO
}


\mprop{}{
	N'importe quel $c\in\R$ peut être vu comme une variable aléatoire constante associée à la fonction valant identiquement $c$.
	De plus, $\E(c) = c$.
}{prop:econst}

\pf{}{
	On a, par définition,
		\[ \E(c) = c P(c = c) = c. \]
	Notons qu'à gauche de l'égalité $c=c$ se trouve une variable aléatoire, alors qu'à droite se trouve un nombre !
	L'événement $c = c$ est vrai pour toutes les issues de l'univers, donc $P(c=c) = P(\Omega) = 1$ car $P$ est une loi de probabilité.
}

\thm{Linéarité de l'espérance}{
	Soient $X, Y$ deux variables aléatoires et $c\in\R$ un réel quelconque.
	Alors
		\begin{align*}
			\E(cX) = c \E(X) && \et && \E(X+Y) = \E(X) + \E(Y).
		\end{align*}
}{thm:linéarité-espérance}

\pf{}{
	On utilise l'exercice \ref{exe:rem-alea} pour alléger les notations.
	\begin{align*}
		\E(cX) = \sum_{\omega\in\Omega} (cX)(\omega) P(\omega) = c \sum_{\omega\in\Omega} X(\omega) P(\omega) = c\E(X).
	\end{align*}
	
	Ainsi
	\begin{align*}
		\E(X+Y) &= \sum_{\omega\in\Omega} (X+Y)(\omega) P(\omega), \\
				&= \sum_{\omega\in\Omega} \bigl[X(\omega) + Y(\omega) \bigr] P(\omega), \\
				&= \sum_{\omega\in\Omega} X(\omega) P(\omega) + \sum_{\omega\in\Omega} Y(\omega) P(\omega), \\
				&= \E(X) + \E(Y).
	\end{align*}
}


\section{Loi de Bernoulli, loi binomiale}

Considérons une expérience aléatoire quelconque ayant deux issue : succès ou échec.
Notons $p\in[0,1]$ la probabilité de succès.

\nomen{
	On appelle \emph{indicatrice} d'un ensemble une variable qui vaut 1 sur cet ensemble, et 0 en dehors.
}

On pose $X$ la variable aléatoire indicatrice de succès :
	\[ X = \begin{cases*} 1 & si succès, \\ 0 & sinon. \end{cases*}. \]
Ainsi $P(X=1) = p$ et $P(X=0) = 1-p$.

\dfn{Loi de Bernoulli}{
	On dit d'une variable aléatoire $X$ qu'elle suit la loi de Bernoulli\footnotemark de paramètre $p\in[0,1]$ dès que
		\begin{align*}
			P(X = 1) = p, && \et && P(X=0) = 1-p.
		\end{align*}
}{dfn:loi-bern}
\footnotetext{Jacques Bernoulli (165?-1705), mathématicien et physicien suisse.}

\notations{
	On note alors $X \sim \Bern(p)$.
}

\exe{}{
	On lance un dé à 6 faces numérotées 1 à 6 bien équilibré et on considère la variable aléatoire $X$ : « 1 si le résultat est inférieur ou égal à 2 ; 0 sinon. »
	$X$ est ainsi la variable indicatrice de l'événement « le résultat est inférieur ou égal à 2 ».
	Montrer que $X \sim \Bern\left(\dfrac13\right)$.
}{exe:bern-dé}{
	Comme $X$ est une variable indicatrice, elle suit une loi de Bernoulli.
	Notons $p=P(X=1)$ son paramètre.
	D'après l'exercice, $p = P\bigl(\text{obtenir 1 ou 2}\bigr) = \dfrac26 = \dfrac13$.
}

\exe{, difficulty=1}{
	On lance une pièce déséquilibrée telle que la probabilité d'obtenir « pile » soit 3 fois la probabilité d'obtenir « face ».
	On considère la variable aléatoire $Y$ indicatrice de « pile ».
	Montrer que $Y \sim \Bern\left(\dfrac34\right)$.
}{exe:bern-dé}{
	Comme $Y$ est une variable indicatrice, elle suit une loi de Bernoulli.
	Notons $p=P(Y=1)$ son paramètre.
	D'après l'exercice, $p = 3P(Y=0) = 3(1-p)$, et donc $p=\dfrac34$.
}

Reconsidérons l'expérience aléatoire à deux issue, succès ou échec, du début de section, où nous avions noté $p\in[0,1]$ la probabilité de succès.
Supposons qu'on répète $n$ fois cette expérience sous réserve de quelques hypothèses fortes :
	\begin{enumerate}[label=\roman*)]
		\item toutes les expériences sont indépendantes : le résultat de l'une n'influe pas le résultat de l'autre ;
		\item toutes les expériences sont réalisées dans des conditions identiques. Par exemple et en particulier, un jet de pièce ne change pas l'équilibre de celle-ci, et la gravité, la pression atmosphérique, la température sont autants de paramètres qui ne changent pas.
	\end{enumerate}

\dfn{}{
	On dit que $X_1, \dots, X_n$ sont indépendantes dès que
		\[ P(X_1 = x_1 \et X_2 = x_2 \et \dots \et X_n = x_n) = P(X_1 = x_1) \cdots P(X_n = x_n). \]
	La probabilité de l'intersection est le produit des probabilités.
}{}

\dfn{}{
	Soient $X_1, \dots, X_n$ des variables aléatoires.
	Dès que
	\begin{enumerate}[label=\roman*)]
		\item les $X_i$ sont indépendantes, et
		\item les $X_i$ suivent la même loi $L$, alors
	\end{enumerate}
	on dit que les $X_i$ sont \emph{indépendantes} et \emph{identitiquement distribuées}.
}{}

\notations{
	On note alors $X_i \overset{\mathrm{iid}}{\sim} L$.
}

\dfn{}{
	On dit que la variable aléatoire $S$ suit la loi binomiale de paramètres $n$ et $p$ dès que
		\[ S = X_1 + \cdots + X_n, \]
	où $X_1, \dots, X_n \overset{\mathrm{iid}}{\sim} \Bern(p)$.
}{}

\notations{
	On note alors $S \sim \Binom(n,p)$.
}

\exe{}{
	Soit $S \sim \Binom(2,p)$.
	Montrer que $P(S=0) = (1-p)^2, P(S=1) = 2p(1-p)$, et $P(S=2) = p^2$.
}{exe:binom2p}{
	Par définition, $S = X_1 + X_2$ où $X_1, X_2 \sim \Bern(p)$.
	Donc $P(S=0) = P(X_1 + X_2 = 0) = P(X_1 = 0 \et X_2 = 0)$.
	Par l'indépendance de $X_1$ et $X_2$, $P(S=0) = P(X_1 = 0) \cdot P(X_2 = 0) = (1-p)^2$.
	
	De la même manière, $P(S=2) = P(X_1 = 1 \et X_2 = 1) = p^2$.
	
	Finalement, $P(S=1) = P\bigl( (X_1 = 0 \et X_2 = 1) \text{ ou } (X_1 = 1 \et X_2 = 0) \bigr) = 2p(1-p)$.
	On aurait également pû trouver cette probabilité en calculant $1-P(S=0)-P(S=2) = 1-p^2 - (1-p)^2 = 2p(1-p)$.
}




